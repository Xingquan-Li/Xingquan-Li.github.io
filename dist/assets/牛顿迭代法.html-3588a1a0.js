import{_ as r}from"./math_17-0395d9a2.js";import{_ as n}from"./plugin-vue_export-helper-c27b6911.js";import{r as l,o as s,c as h,d as a,e as t,a as o,f as i}from"./app-1ed3f6c2.js";const c={},e=i('<h1 id="牛顿迭代法" tabindex="-1"><a class="header-anchor" href="#牛顿迭代法" aria-hidden="true">#</a> 牛顿迭代法</h1><blockquote><p>牛顿法给出了任意方程求根的数值解法，而最优化问题一般会转换为求函数之间在&quot;赋范线性空间&quot;的距离<strong>最小点</strong>，所以，利用牛顿法去求解任意目标函数的<strong>极值点</strong>是个不错的思路。</p></blockquote><h1 id="方程求根" tabindex="-1"><a class="header-anchor" href="#方程求根" aria-hidden="true">#</a> 方程求根</h1><p>对于一元二次方程，求根其实很简单，只要套用求根公式就行了，但找到一个方程的求根公式（<strong>解析解</strong>）其实是很困难的，可以证明5次方程以上便没有解析解了。其他的复杂方程如偏微分方程求解更是超级困难。好在随着计算机技术的发展，解析解变的不再那么重要（至少是在工程上），取而代之的方法便是数值解法，<strong>牛顿法</strong>便是众多数值解法中的一个。<br> 数值法求解又叫做数值分析，主要利用逼近的思想来使数值解通过迭代计算不断接近解析解，而得出来得解就叫做<strong>数值解</strong>，在工程上，数值解只要是在精度要求范围内满足方程便是有用的。</p><h2 id="牛顿迭代法-1" tabindex="-1"><a class="header-anchor" href="#牛顿迭代法-1" aria-hidden="true">#</a> 牛顿迭代法</h2><div style="text-align:center;"><img src="'+r+'" alt="ASIC Flow" width="200"><h4>图1 牛顿迭代法</h4></div><p>sqrt2.png</p>',7),_=a("img",{src:"https://math.jianshu.com/math?formula=x^2-2%3D0",alt:"x^2-2=0"},null,-1),f=a("img",{src:"https://math.jianshu.com/math?formula=\\sqrt 2",alt:"qrt 2"},null,-1),u={href:"https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fkiddie92%2FLearning_Tech%2Fblob%2Fmaster%2F%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-%E7%89%9B%E9%A1%BF%E8%BF%AD%E4%BB%A3%E6%B3%95%2Fequation_sqrt2.py",target:"_blank",rel:"noopener noreferrer"},x=a("img",{src:"https://math.jianshu.com/math?formula=y%3Dx^2-2",alt:"y=x^2-2"},null,-1),p=a("img",{src:"https://math.jianshu.com/math?formula=x",alt:"x"},null,-1),d=a("img",{src:"https://math.jianshu.com/math?formula=x_n",alt:"x_n"},null,-1),g=a("img",{src:"https://math.jianshu.com/math?formula=x_n",alt:"x_n"},null,-1),b=a("img",{src:"https://math.jianshu.com/math?formula=x",alt:"x"},null,-1),v=a("img",{src:"https://math.jianshu.com/math?formula=x_{n%2B1}",alt:"x_{n+1}"},null,-1),j=a("img",{src:"https://math.jianshu.com/math?formula=x_{n%2B1}",alt:"x_{n+1}"},null,-1),D=a("img",{src:"https://math.jianshu.com/math?formula=x_{n}",alt:"x_{n}"},null,-1),k=a("p",{n:""},[t("上述思路的数学表达："),a("br"),t(" 由"),a("img",{src:"https://math.jianshu.com/math?formula=x_{n}",alt:"x_{n}"}),t("计算"),a("img",{src:"https://math.jianshu.com/math?formula=y_{n}",alt:"y_{n}"})],-1),y=i('<figure><img src="https://math.jianshu.com/math?formula=f(x_n)%3Dy_n" alt="f(x_n)=y_n" tabindex="0"><figcaption>f(x_n)=y_n</figcaption></figure><p>得到切线方程：</p><figure><img src="https://math.jianshu.com/math?formula=y-y_n%3D \\left. f(x)&#39; \\right | _{x%3Dx_n}(x-x_n)" alt="y-y_n= eft. f(x)&#39; ight | _{x=x_n}(x-x_n)" tabindex="0"><figcaption>y-y_n= \\left. f(x)&#39; \\right | _{x=x_n}(x-x_n)</figcaption></figure><p>切线和<img src="https://math.jianshu.com/math?formula=x" alt="x">轴的交点，也即，当<img src="https://math.jianshu.com/math?formula=y%3D0" alt="y=0">时，</p><figure><img src="https://math.jianshu.com/math?formula=0-y_n%3D\\left. f(x)&#39; \\right | _{x%3Dx_n}(x-x_n)" alt="0-y_n=eft. f(x)&#39; ight | _{x=x_n}(x-x_n)" tabindex="0"><figcaption>0-y_n=\\left. f(x)&#39; \\right | _{x=x_n}(x-x_n)</figcaption></figure><figure><img src="https://math.jianshu.com/math?formula=\\frac{-y_n}{\\left. f&#39;(x) \\right | _{x%3Dx_n}} %3D (x-x_n)" alt="rac{-y_n}{eft. f&#39;(x) ight | _{x=x_n}} = (x-x_n)" tabindex="0"><figcaption>\\frac{-y_n}{\\left. f&#39;(x) \\right | _{x=x_n}} = (x-x_n)</figcaption></figure><p>当<img src="https://math.jianshu.com/math?formula=\\left. f&#39;(x) \\right | _{x%3Dx_n} \\neq 0" alt="eft. f&#39;(x) ight | _{x=x_n} eq 0">时，</p><figure><img src="https://math.jianshu.com/math?formula=x %3D x_n- \\frac{y_n}{\\left. f&#39;(x) \\right | _{x%3Dx_n}}" alt="x = x_n- rac{y_n}{eft. f&#39;(x) ight | _{x=x_n}}" tabindex="0"><figcaption>x = x_n- \\frac{y_n}{\\left. f&#39;(x) \\right | _{x=x_n}}</figcaption></figure><p>由<img src="https://math.jianshu.com/math?formula=y_n %3D f(x_n)" alt="y_n = f(x_n)">，得到：</p><figure><img src="https://math.jianshu.com/math?formula=x %3D x_n- \\frac{f(x_n)}{\\left. f&#39;(x) \\right | _{x%3Dx_n}}" alt="x = x_n- rac{f(x_n)}{eft. f&#39;(x) ight | _{x=x_n}}" tabindex="0"><figcaption>x = x_n- \\frac{f(x_n)}{\\left. f&#39;(x) \\right | _{x=x_n}}</figcaption></figure><p>令<img src="https://math.jianshu.com/math?formula=x%3Dx_n" alt="x=x_n">，继续迭代，则得到迭代公式：</p><figure><img src="https://math.jianshu.com/math?formula=x_{n%2B1} %3D x_n- \\frac{f(x_n)}{\\left. f&#39;(x) \\right | _{x%3Dx_n}}" alt="x_{n+1} = x_n- rac{f(x_n)}{eft. f&#39;(x) ight | _{x=x_n}}" tabindex="0"><figcaption>x_{n+1} = x_n- \\frac{f(x_n)}{\\left. f&#39;(x) \\right | _{x=x_n}}</figcaption></figure><p>推导过程还可以从<strong>函数泰勒展开的角度</strong>去理解，这在很多博客里有写，这里就不赘述了。</p><p>根据上面的迭代公式，可以计算方程<img src="https://math.jianshu.com/math?formula=x^2-2%3D0" alt="x^2-2=0">的根了：</p><ol><li>猜一个初始值，因为根大概是1点多吧，那就给个<img src="https://math.jianshu.com/math?formula=x_0%3D2" alt="x_0=2">好了；</li><li>计算<img src="https://math.jianshu.com/math?formula=x_1" alt="x_1">：<br><img src="https://math.jianshu.com/math?formula=x_{1} %3D x_0- \\frac{f(x_0)}{\\left. f&#39;(x) \\right | _{x%3Dx_0}}%3D 2- \\frac{f(2)}{\\left. f&#39;(x) \\right | _{x%3D2}}%3D1.5" alt="x_{1} = x_0- rac{f(x_0)}{eft. f&#39;(x) ight | _{x=x_0}}= 2- rac{f(2)}{eft. f&#39;(x) ight | _{x=2}}=1.5"><br><img src="https://math.jianshu.com/math?formula=x_{2} %3D 1.5- \\frac{f(1.5)}{\\left. f&#39;(x) \\right | _{x%3D1.5}}%3D1.416667" alt="x_{2} = 1.5- rac{f(1.5)}{eft. f&#39;(x) ight | _{x=1.5}}=1.416667"><br><img src="https://math.jianshu.com/math?formula=x_{3} %3D 1.416667- \\frac{f(1.416667)}{\\left. f&#39;(x) \\right | _{x%3D1.416667}}%3D1.414216" alt="x_{3} = 1.416667- rac{f(1.416667)}{eft. f&#39;(x) ight | _{x=1.416667}}=1.414216"></li></ol><h2 id="算法优缺点分析" tabindex="-1"><a class="header-anchor" href="#算法优缺点分析" aria-hidden="true">#</a> 算法优缺点分析</h2><p>牛顿法的优点当然就是提供了一种方程求根的数值解方法。而缺点也有几点：</p><ol><li>首先算法是要求函数处处可导的，如果对于优化问题还需要导函数连续（因为要求处处存在二阶导数），否则算法就不能计算函数的根了，比如<img src="https://math.jianshu.com/math?formula=f(x)%3Dx^{1%2F3}" alt="f(x)=x^{1/3}">就不能收敛，虽然函数的根为0，但是它在0处的导数是不存在的；</li><li>求出的解可能仅仅是众多解中的一个，这个比较<strong>依赖于初始值的选取</strong>，比如上面的问题，初始值为2，则收敛到了方程的正数解，要想得到负数解，则需要将初始值选在负数中，现实中的问题，很难去估计解的大小范围；</li><li>如果初始的估计值与根的距离太远收敛就会变的比较慢；</li><li>要求每次迭代是得到的切线导数不能为0，如推导过程所示；</li><li>如果方程本来就没有根，那牛顿法是不能收敛的；</li></ol><h1 id="优化问题求解" tabindex="-1"><a class="header-anchor" href="#优化问题求解" aria-hidden="true">#</a> 优化问题求解</h1><p>优化问题从泛函的角度理解起来，就是计算函数之间的距离最小。对于距离的定义有很多，比较常用的是<strong>二范数</strong>，使<strong>二范数</strong>距离最小的求解过程就叫做最小二乘。对于<img src="https://math.jianshu.com/math?formula=Gm%3Ddata_{predict}" alt="Gm=data_{predict}">这样的线性问题（非线程问题可以通过泰勒展开转换成线性问题），可以定义距离为<img src="https://math.jianshu.com/math?formula=\\phi (m)%3D||Gm-data_{observation}||_2" alt="hi (m)=||Gm-data_{observation}||_2">，为了求距离最小值点，需要先求极值点，问题便转换为求解<img src="https://math.jianshu.com/math?formula=\\phi &#39;(m)%3D0" alt="hi &#39;(m)=0">的根，这时候<strong>牛顿法</strong>便派上了用场。与之前问题不同的是，这里需要求<img src="https://math.jianshu.com/math?formula=\\phi &#39;(m)" alt="hi &#39;(m)">的导数，也即求解<img src="https://math.jianshu.com/math?formula=\\phi &quot;(m)" alt="hi &quot;(m)">，也即Hessian矩阵。假设，此处的参数<img src="https://math.jianshu.com/math?formula=m" alt="m">是n维向量，则Hessian矩阵为：</p>',20),q=a("figure",{pmatrix:""},[a("img",{src:"https://math.jianshu.com/math?formula=H %3D \\begin{pmatrix} \\frac{\\partial ^2f}{\\partial m_1^2} %26 \\frac{\\partial ^2f}{\\partial m_1 \\partial m_2} %26 \\cdots %26 \\frac{\\partial ^2f}{\\partial m_1 \\partial m_n} \\\\ \\frac{\\partial ^2f}{\\partial m_2 \\partial m_1} %26 \\frac{\\partial ^2f}{\\partial m_2^2} %26 \\cdots %26 \\frac{\\partial ^2f}{\\partial m_2 \\partial m_n} \\\\ \\vdots %26 \\vdots %26 \\ddots %26 \\vdots \\\\ \\frac{\\partial ^2f}{\\partial m_n \\partial m_1} %26 \\frac{\\partial ^2f}{\\partial m_n \\partial m_2} %26 \\cdots %26 \\frac{\\partial ^2f}{\\partial m_n^2} \\\\ \\end{pmatrix}",alt:"H = egin{pmatrix} rac{artial ^2f}{artial m_1^2} & rac{artial ^2f}{artial m_1 artial m_2} & dots & rac{artial ^2f}{artial m_1 artial m_n}  rac{artial ^2f}{artial m_2 artial m_1} & rac{artial ^2f}{artial m_2^2} & dots & rac{artial ^2f}{artial m_2 artial m_n}  dots & dots & dots & dots  rac{artial ^2f}{artial m_n artial m_1} & rac{artial ^2f}{artial m_n artial m_2} & dots & rac{artial ^2f}{artial m_n^2}  nd{pmatrix}",tabindex:"0"}),a("figcaption",null,"H = \\begin{pmatrix} \\frac{\\partial ^2f}{\\partial m_1^2} & \\frac{\\partial ^2f}{\\partial m_1 \\partial m_2} & \\cdots & \\frac{\\partial ^2f}{\\partial m_1 \\partial m_n} \\ \\frac{\\partial ^2f}{\\partial m_2 \\partial m_1} & \\frac{\\partial ^2f}{\\partial m_2^2} & \\cdots & \\frac{\\partial ^2f}{\\partial m_2 \\partial m_n} \\ \\vdots & \\vdots & \\ddots & \\vdots \\ \\frac{\\partial ^2f}{\\partial m_n \\partial m_1} & \\frac{\\partial ^2f}{\\partial m_n \\partial m_2} & \\cdots & \\frac{\\partial ^2f}{\\partial m_n^2} \\ \\end")],-1),H=i(`<p>所以，牛顿法求解最优化问题，需要先求目标函数的Jacobian矩阵和Hessian矩阵，计算量比较大的便是计算Hessian矩阵了，因为二阶导计算量成指数增长。</p><blockquote><p>注意，这里若二阶导数是连续的，则<img src="https://math.jianshu.com/math?formula=H" alt="H">是对称矩阵。</p></blockquote><h2 id="算法步骤" tabindex="-1"><a class="header-anchor" href="#算法步骤" aria-hidden="true">#</a> 算法步骤</h2><p><strong>步骤1：</strong> 给定误差阈值<img src="https://math.jianshu.com/math?formula=0\\leq\\epsilon &lt;&lt;1" alt="0eqpsilon &lt;&lt;1">，初始模型<img src="https://math.jianshu.com/math?formula=m_0" alt="m_0">（也可以给定迭代次数）；<br><strong>步骤2：</strong> 计算梯度<img src="https://math.jianshu.com/math?formula=g_k%3D\\nabla f(m_k)" alt="g_k=abla f(m_k)">，若<img src="https://math.jianshu.com/math?formula=g_k\\leq \\epsilon" alt="g_keq psilon">，停止计算，输出<img src="https://math.jianshu.com/math?formula=m^* \\approx m_k" alt="m^* pprox m_k">;<br><strong>步骤3：</strong> 计算Hessian矩阵<img src="https://math.jianshu.com/math?formula=G_k%3D\\nabla^2f(m_k)" alt="G_k=abla^2f(m_k)">，计算<img src="https://math.jianshu.com/math?formula=d_k%3D\\frac {g_k}{G_k}" alt="d_k=rac {g_k}{G_k}">;<br><strong>步骤4：</strong> 令<img src="https://math.jianshu.com/math?formula=x_{k%2B1}%3Dx_{k}-d_{k}" alt="x_{k+1}=x_{k}-d_{k}">，k=k+1，转到第2步。</p><h2 id="示例" tabindex="-1"><a class="header-anchor" href="#示例" aria-hidden="true">#</a> 示例</h2><p>一个例子：求极小值<img src="https://math.jianshu.com/math?formula=f(m_1%2Cm_2) %3D -m_1^3-m_2^3%2B3m_1^2%2B2m_2^2%2Bm_1%2Bm_2-1" alt="f(m_1,m_2) = -m_13-m_23+3m_12+2m_22+m_1+m_2-1"></p><div class="language-C++ line-numbers-mode" data-ext="C++"><pre class="language-C++"><code>
#include &lt;stdio.h&gt;
#include &lt;math.h&gt;

double funY(double x);
double funY1(double x);

int main() {
    double x, x1, x2;

    x1 = 1.5;//求1.5附近的根
    x2 = x1 - funY(x1) / funY1(x1);
    while (fabs(x2 - x1) &gt; 1e-6) {
        x1 = x2;
        x2 = x2 = x1 - funY(x1) / funY1(x1);
    }
    printf(&quot;%lf&quot;,x2);
}
//————————————————————
//  y的函数
double funY(double x) {
    double y;
    y = 2 * x*x*x - 4 * x*x + 3 * x - 6;
    return y;
}
//y的一阶导数
double funY1(double x) {
    double y1;
    y1 = 6 * x*x - 8 * x + 3;
    return y1;
}

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h1 id="几个改进方法" tabindex="-1"><a class="header-anchor" href="#几个改进方法" aria-hidden="true">#</a> 几个改进方法</h1><p>优化算化考虑重点包括算法的通用性、有效性、收敛性、效率，当然，这些都包括在时间复杂度和空间复杂度中。牛顿法存在几个问题需要考虑一下：</p><ol><li>计算Hessian矩阵太耗资源和时间了；</li><li>牛顿法不稳定，只有<img src="https://math.jianshu.com/math?formula=H" alt="H">正定时才收敛，也即要求目标函数的 Hessian 阵 <img src="https://math.jianshu.com/math?formula=H_{i%2Cj}" alt="H_{i,j}"> 在每个迭代点 <img src="https://math.jianshu.com/math?formula=m_i" alt="m_i"> 处是正定的，否则难以保证牛顿法收敛的方向，实际上，<img src="https://math.jianshu.com/math?formula=H" alt="H">很可能是一个病态/奇异矩阵；</li><li>初始模型<img src="https://math.jianshu.com/math?formula=m_0" alt="m_0">很重要，选的不好会迭代很多次，收敛比较慢；</li><li>初始模型的选取不在最小值附近，很容易让结果陷入局部极小值。</li></ol><p>对此，大牛们提出了一些改进的方法：</p><ol><li>拟牛顿法：为了避免计算Hessian矩阵，不直接计算<img src="https://math.jianshu.com/math?formula=H" alt="H">，而是构造一个矩阵<img src="https://math.jianshu.com/math?formula=K" alt="K">来近似，<img src="https://math.jianshu.com/math?formula=K" alt="K">需要一直<strong>正定</strong>并且<strong>更新起来比较简单</strong>，此处可以查看相关文献，不赘述了；</li><li>高斯牛顿法：将目标函数<img src="https://math.jianshu.com/math?formula=\\phi (m)%3D||Gm-data_{observation}||_2" alt="hi (m)=||Gm-data_{observation}||_2">变换为<img src="https://math.jianshu.com/math?formula=\\phi (m)%3D\\frac {1}{2}||r(m)||_2" alt="hi (m)=rac {1}{2}||r(m)||_2">，其中<img src="https://math.jianshu.com/math?formula=r(m)" alt="r(m)">表示残差（residual），则根据chain rule，可以得到：<br><img src="https://math.jianshu.com/math?formula=\\nabla^2 \\phi (m)%3D\\nabla r(m) \\nabla^T r(m)%2B\\sum_{i%3D1}^nr_i(m)\\nabla^2 r_{i}(m)" alt="abla^2 hi (m)=abla r(m) abla^T r(m)+um_{i=1}nr_i(m)\\nabla2 r_{i}(m)"><br> 这里令<img src="https://math.jianshu.com/math?formula=Q(m)%3D\\sum_{i%3D1}^nr_i(m)\\nabla^2 r_{i}(m)" alt="Q(m)=um_{i=1}nr_i(m)\\nabla2 r_{i}(m)">，若对于将要迭代的值<img src="https://math.jianshu.com/math?formula=m^*" alt="m^*">，有<img src="https://math.jianshu.com/math?formula=r_{i}(m^*)%3D0" alt="r_{i}(m^*)=0">则<img src="https://math.jianshu.com/math?formula=Q(m)%3D0" alt="Q(m)=0">；这样的话就不需要计算Hessian矩阵了。这个想法不错，当<img src="https://math.jianshu.com/math?formula=m^*" alt="m^*">和极值点/最小值的距离比较近时，简直完美；但是，当初始值距离最小值较远时，<img src="https://math.jianshu.com/math?formula=Q(m) \\approx 0" alt="Q(m) pprox 0">的思路就不行了，此时，高斯-牛顿法并不收敛。</li></ol><blockquote><p>所以高斯-牛顿法也是极度依赖初始模型/初值的选取的</p></blockquote><ol><li>莱文贝格－马夸特方法(Levenberg–Marquardt algorithm)：该方法结合了高斯-牛顿法和最速下降法/梯度法，因为高斯-牛顿法比较依赖初始模型/初值，梯度法可以克服这个问题；而梯度法收敛速度要低于高斯-牛顿法，所以该方法能提供数非线性最小化（局部最小）的数值解。其实做法也很简单，就是在目标函数内加了一个参数<img src="https://math.jianshu.com/math?formula=\\lambda" alt="ambda">，所以该方法也叫做阻尼最小二乘法。类似的做法在Tikhonov正则化中也出现了。</li></ol><blockquote><p>所有这些方法都可能陷入局部极小值，而非找到全局极小值/最小值。要想克服这个问题，就需要启发式/非线性优化算法了。</p></blockquote>`,15);function B(E,F){const m=l("ExternalLinkIcon");return s(),h("div",null,[e,a("p",null,[t("先考虑一个小问题：求解方程"),_,t("的根，也即求解"),f,t("。牛顿迭代法的思想从几何的角度很好理解，如上图所示（画图的脚本在"),a("a",u,[t("这里"),o(m)]),t("）方程的根就是函数"),x,t("与"),p,t("轴的交点处横坐标的值。从图中"),d,t("点出发，计算函数在"),g,t("点处的切线，再计算切线和"),b,t("轴的交点得到"),v,t("，再计算函数在"),j,t("点处的切线... 一直这样迭代下去，可以发现"),D,t("会越来越接近方程的根。")]),k,y,q,H])}const A=n(c,[["render",B],["__file","牛顿迭代法.html.vue"]]);export{A as default};
